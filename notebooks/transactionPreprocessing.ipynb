{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 21:58:05 WARN Utils: Your hostname, DESKTOP-3NQ3PQI resolves to a loopback address: 127.0.1.1; using 172.17.23.167 instead (on interface eth0)\n",
      "22/10/05 21:58:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 21:58:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Imputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.driver.memory\", '8g')\n",
    "    .config(\"spark.executor.memory\", '8g')\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weights_sdf = spark.read.parquet(\n",
    "    '../data/curated/demographic_weights.parquet'\n",
    ")\n",
    "consumers_sdf = spark.read.parquet(\n",
    "    '../data/curated/cleaned_consumers.parquet'\n",
    ")\n",
    "\n",
    "user_details_sdf = spark.read.parquet(\n",
    "    '../data/tables/consumer_user_details.parquet'\n",
    ")\n",
    "\n",
    "merchants_sdf = spark.read.parquet(\n",
    "    '../data/curated/merchants.parquet'\n",
    ")\n",
    "\n",
    "consumer_fraud_prob_sdf = spark.read.option('header', True).csv(\n",
    "    '../data/tables/consumer_fraud_probability.csv'\n",
    ").withColumn(\n",
    "    'order_datetime',\n",
    "    F.to_date('order_datetime')\n",
    ").withColumn(\n",
    "    'fraud_probability',\n",
    "    F.col('fraud_probability')/100\n",
    ").withColumnRenamed(\n",
    "    'fraud_probability',\n",
    "    'consumer_fraud_prob'\n",
    ")\n",
    "\n",
    "merchant_fraud_prob_sdf = spark.read.option('header', True).csv(\n",
    "    '../data/tables/merchant_fraud_probability.csv'\n",
    ").withColumn(\n",
    "    'order_datetime',\n",
    "    F.to_date('order_datetime')\n",
    ").withColumn(\n",
    "    'fraud_probability',\n",
    "    F.col('fraud_probability')/100\n",
    ").withColumnRenamed(\n",
    "    'fraud_probability',\n",
    "    'merchant_fraud_prob'\n",
    ")\n",
    "\n",
    "postcode_poa_sdf = spark.read.parquet(\n",
    "    '../data/curated/census/postcode_poa.parquet'\n",
    ")\n",
    "\n",
    "transactions_sdf = spark.read.parquet(\n",
    "    '../data/tables/transactions_20210228_20210827_snapshot'\n",
    ").union(\n",
    "    spark.read.parquet(\n",
    "        '../data/tables/transactions_20210828_20220227_snapshot'\n",
    "    )\n",
    ").union(\n",
    "    spark.read.parquet(\n",
    "        '../data/tables/transactions_20220228_20220828_snapshot'\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove transactions outside valid bnpl range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transactions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14195505"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total transactions:')\n",
    "transactions_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total removed:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1363555"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round to 2 decimal places, and define a transaction range\n",
    "min_value = 10\n",
    "max_value = 10000\n",
    "pre_count = transactions_sdf.count()\n",
    "\n",
    "transactions_sdf = transactions_sdf.where(\n",
    "    (F.col('dollar_value') >= min_value)\n",
    "    & (F.col('dollar_value') <= max_value)\n",
    ")\n",
    "\n",
    "print('Total removed:')\n",
    "pre_count - transactions_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join transaction data with consumer data and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join with consumer data (weights and fraud probabilities)\n",
    "transactions_sdf = transactions_sdf.join(\n",
    "    user_details_sdf,\n",
    "    on = 'user_id',\n",
    "    how = 'left'\n",
    ").join(\n",
    "    consumers_sdf.select(\n",
    "        'consumer_id', 'postcode', 'gender'\n",
    "    ),\n",
    "    on = 'consumer_id',\n",
    "    how = 'left'\n",
    ").join(\n",
    "    postcode_poa_sdf,\n",
    "    on = 'postcode',\n",
    "    how = 'left'\n",
    ").join(\n",
    "    weights_sdf,\n",
    "    on = ['poa' ,'gender'],\n",
    "    how = 'left'\n",
    ").join(\n",
    "    consumer_fraud_prob_sdf,\n",
    "    on = ['user_id', 'order_datetime'],\n",
    "    how = 'left'\n",
    ").na.fill(\n",
    "    0, \n",
    "    subset = 'consumer_fraud_prob'\n",
    ")\n",
    "\n",
    "# Impute null weights with column average\n",
    "imputer = Imputer(inputCol = 'weight', outputCol='weight', strategy = 'mean')\n",
    "transactions_sdf = imputer.fit(transactions_sdf).transform(transactions_sdf)\n",
    "\n",
    "# Apply transaction level weighting\n",
    "transactions_sdf = transactions_sdf.withColumn(\n",
    "    'weighted_dollar_value',\n",
    "    F.col('dollar_value')*F.col('weight')*(1 - F.col('consumer_fraud_prob'))\n",
    ")\n",
    "\n",
    "# Group by merchant and day and join with merchant data\n",
    "transactions_sdf = transactions_sdf.groupby(\n",
    "    'merchant_abn', 'order_datetime'\n",
    ").agg(\n",
    "    F.sum('weighted_dollar_value').alias('weighted_dollar_value'),\n",
    "    F.sum('dollar_value').alias('dollar_value')\n",
    ").join(\n",
    "    merchants_sdf.select(\n",
    "        'merchant_abn', 'take_rate'\n",
    "    ),\n",
    "    on = 'merchant_abn',\n",
    ").join(\n",
    "    merchant_fraud_prob_sdf,\n",
    "    on = ['merchant_abn', 'order_datetime'],\n",
    "    how = 'left'\n",
    ").na.fill(\n",
    "    0,\n",
    "    subset = 'merchant_fraud_prob'\n",
    ")\n",
    "\n",
    "# Apply merchant level weighting\n",
    "transactions_sdf = transactions_sdf.withColumn(\n",
    "    'weighted_dollar_value',\n",
    "    F.col('weighted_dollar_value')*F.col('take_rate')*(1 - F.col('merchant_fraud_prob'))\n",
    ").select(\n",
    "    'merchant_abn', 'order_datetime', 'weighted_dollar_value', 'dollar_value'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_df = transactions_sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = transactions_df[['order_datetime']].drop_duplicates()\n",
    "merchants = transactions_df[['merchant_abn']].drop_duplicates()\n",
    "time_steps['key'] = 1\n",
    "merchants['key'] = 1\n",
    "merchant_time_steps = pd.merge(\n",
    "    merchants,\n",
    "    time_steps,\n",
    "    on = 'key'\n",
    ").drop('key', axis = 1)\n",
    "\n",
    "transactions_df = pd.merge(\n",
    "    transactions_df,\n",
    "    merchant_time_steps,\n",
    "    on = ['merchant_abn', 'order_datetime'],\n",
    "    how = 'outer'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df['week_idx'] = (\n",
    "    (\n",
    "        transactions_df['order_datetime'] - transactions_df['order_datetime'].min()\n",
    "    )/np.timedelta64(1, 'W')\n",
    ").astype(int)\n",
    "\n",
    "transactions_df = transactions_df[transactions_df['week_idx'] != transactions_df['week_idx'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = transactions_df.groupby(\n",
    "    ['merchant_abn', 'week_idx']\n",
    ").agg({'order_datetime' : 'min', 'weighted_dollar_value' : 'sum', 'dollar_value' : 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df['order_datetime'] = pd.to_datetime(transactions_df['order_datetime'])\n",
    "transactions_df['week_of_year'] = transactions_df['order_datetime'].dt.isocalendar().week.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.to_parquet('../data/curated/weighted_transactions.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
