{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following code is for external data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 10:41:53 WARN Utils: Your hostname, DESKTOP-3NQ3PQI resolves to a loopback address: 127.0.1.1; using 172.31.183.205 instead (on interface eth0)\n",
      "22/09/19 10:41:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 10:41:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2 Preprocessing\")\n",
    "    .config(\"spark.driver.memory\", '4g')\n",
    "    .config(\"spark.executor.memory\", '8g')\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Postcode to ABS Postal Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "postal_areas_gdf = gpd.read_file('../data/raw/postcodes/abs_postal_areas.zip')\n",
    "consumer_details_df = pd.read_csv('../data/tables/tbl_consumer.csv', delimiter=\"|\")\n",
    "postcode_df = pd.read_csv('../data/raw/postcodes/postcodes.csv').drop_duplicates('postcode')\n",
    "\n",
    "# Convert postcode dataframe to geodataframe\n",
    "postcode_gdf = gpd.GeoDataFrame(\n",
    "    postcode_df, geometry=gpd.points_from_xy(postcode_df['long'], postcode_df['lat'])\n",
    ")\n",
    "postcode_gdf.crs = postal_areas_gdf.crs\n",
    "\n",
    "# Get list of postcodes not listed as abs postal areas and filter geodataframe to just these postcodes\n",
    "unmapped = consumer_details_df[~consumer_details_df['postcode'].astype(str).str.zfill(4).isin(postal_areas_gdf['POA_CODE21'])]['postcode'].unique()\n",
    "postcodes_gdf = postcode_gdf[postcode_gdf['postcode'].isin(unmapped)]\n",
    "\n",
    "# Spatially join unmapped postcodes and abs postal areas\n",
    "postcode_poa_gdf = postcodes_gdf.sjoin(postal_areas_gdf, how = 'inner')\n",
    "postcode_poa_df = postcode_poa_gdf[['postcode', 'POA_CODE21']]\n",
    "postcode_poa_df = postcode_poa_df.rename(columns = {'POA_CODE21' : 'poa'}).reset_index()\n",
    "postcode_poa_df = pd.concat([postcode_poa_df,pd.DataFrame(data = {'postcode' : postal_areas_gdf['POA_CODE21'], 'poa' : postal_areas_gdf['POA_CODE21']})], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>postcode</th>\n",
       "      <th>poa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4308.0</td>\n",
       "      <td>2608</td>\n",
       "      <td>2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4315.0</td>\n",
       "      <td>2610</td>\n",
       "      <td>2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>801</td>\n",
       "      <td>0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>804</td>\n",
       "      <td>0820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7469</td>\n",
       "      <td>7469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7470</td>\n",
       "      <td>7470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9494</td>\n",
       "      <td>9494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9797</td>\n",
       "      <td>9797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZZ</td>\n",
       "      <td>ZZZZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3169 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index postcode   poa\n",
       "0        0.0      200  2601\n",
       "1     4308.0     2608  2601\n",
       "2     4315.0     2610  2601\n",
       "3        4.0      801  0800\n",
       "4        5.0      804  0820\n",
       "...      ...      ...   ...\n",
       "3164     NaN     7469  7469\n",
       "3165     NaN     7470  7470\n",
       "3166     NaN     9494  9494\n",
       "3167     NaN     9797  9797\n",
       "3168     NaN     ZZZZ  ZZZZ\n",
       "\n",
       "[3169 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postcode_poa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 postcodes could not be mapped. Niether of these could be found in the Australia post website. https://postcodes-australia.com/postcodes/6958 says 6958 is a Western Australian postcode reserved for non standard use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_sdf = spark.read.parquet(\n",
    "    '../data/tables/transactions_20210228_20210827_snapshot/'\n",
    ").union(\n",
    "    spark.read.parquet(\n",
    "        '../data/tables/transactions_20210828_20220227_snapshot/'\n",
    "    )\n",
    ").union(\n",
    "    spark.read.parquet(\n",
    "        '../data/tables/transactions_20220228_20220828_snapshot/'\n",
    "    )\n",
    ")\n",
    "\n",
    "ids_sdf = spark.read.parquet(\n",
    "    '../data/tables/consumer_user_details.parquet'\n",
    ")\n",
    "\n",
    "consumers_sdf = spark.read.options(\n",
    "    header = True, delimiter = '|'\n",
    ").csv(\n",
    "    '../data/tables/tbl_consumer.csv'\n",
    ")\n",
    "\n",
    "age_sdf = spark.read.options(\n",
    "    header = True\n",
    ").csv(\n",
    "    '../data/curated/census/age_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_sdf.sample(0.01).write.parquet('../data/raw/samples/transaction_sample.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_sdf = spark.read.parquet('../data/raw/samples/transaction_sample.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creates dataframe grouped by merchant and postcode with propn of customers for each corresponding postcode\n",
    "merchants_sdf = transactions_sdf.join(\n",
    "    ids_sdf,\n",
    "    on = 'user_id'\n",
    ").join(\n",
    "    consumers_sdf,\n",
    "    on = 'consumer_id'\n",
    ").groupBy(\n",
    "    'merchant_abn', 'postcode'\n",
    ").count().join(\n",
    "    transactions_sdf.groupby(\n",
    "        'merchant_abn'\n",
    "    ).count().withColumnRenamed(\n",
    "        'count',\n",
    "        'size'\n",
    "    ),\n",
    "    on = 'merchant_abn'\n",
    ").withColumn(\n",
    "    'propn',\n",
    "    F.col('count')/F.col('size')\n",
    ").drop(\n",
    "    'count',\n",
    "    'size'\n",
    ")\n",
    "\n",
    "# Joins merchant and postcode data with abs data for population by age\n",
    "merchants_sdf = merchants_sdf.join(\n",
    "    age_sdf,\n",
    "    on = 'postcode'\n",
    ")\n",
    "\n",
    "# Creates scaled version of each population metric by age\n",
    "for col in age_sdf.columns:\n",
    "    if col == 'postcode':\n",
    "        continue\n",
    "    merchants_sdf = merchants_sdf.withColumn(\n",
    "        col+'_scaled',\n",
    "        F.col(col)*F.col('propn')\n",
    "    )\n",
    "\n",
    "# Removes non scaled columns (used to make the scaled columns) and calculates weighted sum of each population metric by propn of customers from that postcode\n",
    "merchants_sdf = merchants_sdf.select(\n",
    "    merchants_sdf.colRegex(\"`merchant_abn|.*_scaled`\")\n",
    ").groupBy(\n",
    "    'merchant_abn'\n",
    ").sum()\n",
    "\n",
    "merchants_df = merchants_sdf.toPandas()\n",
    "\n",
    "# Renames columns and sets index\n",
    "merchants_df = merchants_df.drop(\n",
    "    'sum(merchant_abn)',\n",
    "    axis = 1\n",
    ").set_index(\n",
    "    'merchant_abn'\n",
    ").rename(\n",
    "    columns = {col : col[4:-1] for col in merchants_df.columns}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates median of each row in dataframe where each row corresponds to a count of the given column value\n",
    "def get_median_col(df):\n",
    "    median_count = df.sum(axis = 1)/2\n",
    "\n",
    "    return df.cumsum(axis = 1).apply(\n",
    "        lambda col : (col > median_count)\n",
    "    ).idxmax(\n",
    "        axis = 1\n",
    "    )\n",
    "\n",
    "# Executes get_medial_col function for males females and persons    \n",
    "for person_type in ['m', 'f', 'p']:\n",
    "    merchants_df[f'median_age_{person_type}'] = get_median_col(\n",
    "        merchants_df.filter(\n",
    "            regex = f'age_yr_(\\d+|(80_84)|(85_89)|(90_94)|(95_99)|(100_yr_over))_{person_type}_scaled',\n",
    "            axis = 1\n",
    "        )\n",
    "    ).apply(\n",
    "        lambda x : re.findall('\\d+', x)[0]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_sdf = spark.read.option(\"header\", \"true\").csv(\"../data/raw/external/income.csv\")\n",
    "external_sdf= external_sdf.withColumnRenamed(\n",
    "    \"INCP Total Personal Income (weekly)\",\n",
    "    \"Income\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_factors = list(set(external_sdf.select(F.collect_list(\"Income\")).first()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there seems to be a row per location regarding the total amount of 'Count'. We wish to extract this information and create a separate dataset for easier access to these numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_total = external_sdf.filter(F.col(\"Income\") == \"Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_sdf = external_sdf.where(F.col(\"Income\") != \"Total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use regular expression to find amount range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = external_sdf.select('Income').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readline import append_history_file\n",
    "\n",
    "\n",
    "output_col = []\n",
    "pattern = \"\\((\\$\\d*,?\\d+-\\$\\d*,*\\d*)|(\\$\\d*,?\\d* or more)\\)\"\n",
    "\n",
    "for income in temp_df[\"Income\"]:\n",
    "    matched = re.findall(pattern, income)\n",
    "    if len(matched) > 0:\n",
    "        output_col.append(matched[0][0])\n",
    "    else:\n",
    "        output_col.append(income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['Income Parsed'] = output_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.createDataFrame(temp_df)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ben's preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = pd.read_csv('../data/raw/external/income.csv')\n",
    "income_df = income_df.drop(index=range(len(income_df) - 4, len(income_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = income_df.rename(columns = {\"SA2 (UR)\" : \"Region\", \"INCP Total Personal Income (weekly)\" : \"Income\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_income(row):\n",
    "    if row['Income'] in ['Negative income', 'Nil income', 'Total', 'Not stated', 'Not applicable']:\n",
    "        row['weekly_income'] = row['Income']\n",
    "        row['yearly_income'] = row['Income']\n",
    "    else:\n",
    "        matches = re.findall('(.*)\\s\\((.*)\\)', row['Income'])\n",
    "        row['weekly_income'] = matches[0][0]\n",
    "        row['yearly_income'] = matches[0][1]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = income_df.apply(convert_income, axis = 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
