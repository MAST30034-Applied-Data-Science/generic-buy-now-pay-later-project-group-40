{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.virtualenvs/ads_proj2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import EncoderNormalizer, GroupNormalizer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import NormalDistributionLoss\n",
    "from pytorch_forecasting.models.deepar import DeepAR\n",
    "\n",
    "warnings.simplefilter(\"error\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('../../generic-buy-now-pay-later-project-group-40/data/curated/weighted_transactions.parquet', )\n",
    "data['order_datetime'] = pd.to_datetime(data['order_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 0 values for days on which merchants had no transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = data[['order_datetime']].drop_duplicates()\n",
    "merchants = data[['merchant_abn']].drop_duplicates()\n",
    "time_steps['key'] = 1\n",
    "merchants['key'] = 1\n",
    "merchant_time_steps = pd.merge(\n",
    "    merchants,\n",
    "    time_steps,\n",
    "    on = 'key'\n",
    ").drop('key', axis = 1)\n",
    "\n",
    "data = pd.merge(\n",
    "    data,\n",
    "    merchant_time_steps,\n",
    "    on = ['merchant_abn', 'order_datetime'],\n",
    "    how = 'outer'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add time_idx column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time_idx'] = (\n",
    "    (\n",
    "        data['order_datetime'].sort_values() - data['order_datetime'].min()\n",
    "    )/np.timedelta64(1, 'D')\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add monthofyear and dayofweek columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['week_of_year'] = data['order_datetime'].dt.isocalendar().week.astype(str)\n",
    "data['day_of_week'] = data['order_datetime'].dt.dayofweek.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 100 merchants to test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_merchants = data['merchant_abn'].sample(100, random_state = 100)\n",
    "data = data[data['merchant_abn'].isin(selected_merchants)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 365\n",
    "max_encoder_length = int(data['time_idx'].max())\n",
    "#training_cutoff = data[\"time_idx\"].max() - max_prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'validation.save(\"validation.pkl\")'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"weighted_dollar_value\",\n",
    "    group_ids=[\"merchant_abn\"],\n",
    "    min_encoder_length=max_encoder_length//2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_categoricals = [\"day_of_week\", \"week_of_year\"],\n",
    "    time_varying_unknown_reals=[\"weighted_dollar_value\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"merchant_abn\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    randomize_length=None,\n",
    ")\n",
    "\n",
    "\"\"\"validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    data,\n",
    "    predict=True,\n",
    "    stop_randomization=True\n",
    ")\"\"\"\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=8)\n",
    "\"\"\"val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\"\"\"\n",
    "\n",
    "# save datasets\n",
    "training.save(\"training.pkl\")\n",
    "\"\"\"validation.save(\"validation.pkl\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 17.0k\n"
     ]
    }
   ],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"train_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    #limit_val_batches=3,\n",
    "    # fast_dev_run=True,\n",
    "    # logger=logger,\n",
    "    # profiler=True,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    accelerator = 'cpu'\n",
    ")\n",
    "\n",
    "\n",
    "deepar = DeepAR.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.1,\n",
    "    hidden_size=32,\n",
    "    dropout=0.1,\n",
    "    loss=NormalDistributionLoss(),\n",
    "    log_interval=10,\n",
    "    #log_val_interval=3,\n",
    "    # reduce_on_plateau_patience=3,\n",
    ")\n",
    "print(f\"Number of parameters in network: {deepar.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 815   \n",
      "3 | rnn                    | LSTM                   | 16.1 K\n",
      "4 | distribution_projector | Linear                 | 66    \n",
      "------------------------------------------------------------------\n",
      "17.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.0 K    Total params\n",
      "0.068     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [03:44<00:00,  7.50s/it, loss=0.834, v_num=4, train_loss_step=0.652, train_loss_epoch=0.873]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [03:44<00:00,  7.50s/it, loss=0.834, v_num=4, train_loss_step=0.652, train_loss_epoch=0.873]\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(10)\n",
    "trainer.fit(\n",
    "    deepar,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    #val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data to predict next 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_data = data[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
    "\n",
    "last_data = data[lambda x: x.time_idx == x.time_idx.max()]\n",
    "\n",
    "decoder_data = pd.concat(\n",
    "    [last_data.assign(order_datetime=lambda x: x.order_datetime + pd.offsets.Day(i)) for i in range(1, max_prediction_length + 1)],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# add time index consistent with \"data\"\n",
    "decoder_data['time_idx'] = (\n",
    "    (\n",
    "        decoder_data['order_datetime'].sort_values() - decoder_data['order_datetime'].min()\n",
    "    )/np.timedelta64(1, 'D')\n",
    ").astype(int)\n",
    "\n",
    "decoder_data[\"time_idx\"] += encoder_data[\"time_idx\"].max() + 1 - decoder_data[\"time_idx\"].min()\n",
    "\n",
    "# adjust additional time feature(s)\n",
    "decoder_data['week_of_year'] = decoder_data['order_datetime'].dt.isocalendar().week.astype(str)\n",
    "decoder_data['day_of_week'] = decoder_data['order_datetime'].dt.dayofweek.astype(str)\n",
    "# combine encoder and decoder data\n",
    "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 10 example merchant predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions, new_x = deepar.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    deepar.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert output to df and write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, index = deepar.predict(new_raw_predictions, mode=\"prediction\", return_index=True)\n",
    "\n",
    "predictions_df = pd.DataFrame(\n",
    "    predictions.numpy()\n",
    ").reset_index().melt(\n",
    "    id_vars = 'index', var_name = 'rel_time_idx'\n",
    ")\n",
    "\n",
    "index_df = index.reset_index()\n",
    "\n",
    "predictions_df = pd.merge(\n",
    "    index_df,\n",
    "    predictions_df,\n",
    "    on = 'index'\n",
    ")\n",
    "\n",
    "predictions_df['time_idx'] = predictions_df['time_idx'] + predictions_df['rel_time_idx']\n",
    "\n",
    "predictions_df = predictions_df.rename(columns = {'index' : 'merchant_idx'}).drop('rel_time_idx', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_parquet('../data/curated/transaction_predictions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92ea58c70db46728db75825e7170aaba7ae15ca2d0e36751b515e78ab3619849"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('ads_proj2': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
